{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QZjx03reMU5"
   },
   "source": [
    "#Import the required libraries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jddSFp3j_hAs"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import VectorDBQA, RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "import openai\n",
    "import os\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUCYUMnLgvJ5"
   },
   "source": [
    "# Loading Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9S0HlZhW_ofN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of split documents: 20\n",
      "Chunk 1: that goes beyond the traditional aspects of \n",
      "collective bargaining in order to share knowledge \n",
      "and to jointly find opportunities related to \n",
      "important matters such as Creating Shared Value, \n",
      "the heal...\n",
      "Chunk 2: minimal levels of management and broad spans \n",
      "of control, which enable people development, \n",
      "increase efficiency, and ease implementation \n",
      "of our “Nestlé Management and Leadership \n",
      "Principles”.\n",
      "Less hi...\n",
      "Chunk 3: A dynamic organisation creates a climate \n",
      "of innovation and allows people to think from \n",
      "different perspectives. At Nestlé we encourage \n",
      "our people to take risks. Mistakes may be made \n",
      "but there is al...\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader('https://www.nestle.com/sites/default/files/asset-library/documents/jobs/the_nestle_hr_policy_pdf_2012.pdf')\n",
    "documents = loader.load()\n",
    "\n",
    "# initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, # Maximum size of each chunk\n",
    "    chunk_overlap=200 # Overlap between chunks to preserve context\n",
    ")\n",
    "\n",
    "# Split the loaded documents\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# verify the split documents\n",
    "print(f\"Number of split documents: {len(split_documents)}\")\n",
    "# preview the last 3 chunks\n",
    "for i, doc in enumerate(split_documents[-3:]):  # Preview last 3 chunks\n",
    "    print(f\"Chunk {i+1}: {doc.page_content[:200]}...\")  # Print first 200 characters of each chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oH5NQqp_in_T"
   },
   "source": [
    " # Creating Vector Representation of Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5bY_6TK_yFC",
    "outputId": "b509cdcd-f683-474d-fe5b-e9a253f9ba99"
   },
   "outputs": [],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "# number of chunks for cheaper embedding\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectordb = Chroma.from_documents(texts, embeddings)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2CP1tCAiyTS"
   },
   "source": [
    "# Setting Up Question-Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JKMZGJeWi17S"
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\"), chain_type=\"stuff\", retriever=vectordb.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVU8cmrSi6Da"
   },
   "source": [
    "# Defining Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "M4CETG52_0yR"
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Define the prompt template in English\n",
    "template = \"\"\"\n",
    "I am a HR helpful assistant. Please answer the following question in English.\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create the PromptTemplate instance with the modified English template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APP-VcVNjG6C"
   },
   "source": [
    "#  Building Chat Interface with Gradio and Launching the Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "id": "OIPOs0CD_4G1",
    "outputId": "944a231d-3729-466f-d0c1-af33b50cecda"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgr\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_text\u001b[39m(history, text):\n\u001b[32m      4\u001b[39m     history = history + [(text, \u001b[38;5;28;01mNone\u001b[39;00m)]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def add_text(history, text):\n",
    "    history = history + [(text, None)]\n",
    "    return history, \"\"\n",
    "\n",
    "def bot(history):\n",
    "    query = history[-1][0]\n",
    "    query = prompt.format(question=query)\n",
    "    answer = qa.run(query)\n",
    "    source = qa._get_docs(query)[0]\n",
    "    source_sentence = source.page_content\n",
    "    answer_source = source_sentence +\"\\n\"+\"source:\"+source.metadata[\"source\"] + \", page:\" + str(source.metadata[\"page\"])\n",
    "    history[-1][1] = answer # + answer_source\n",
    "    return history\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot([], elem_id=\"chatbot\").style(height=400)\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=0.6):\n",
    "            txt = gr.Textbox(\n",
    "                show_label=False,\n",
    "                placeholder=\"Enter text and press enter\",\n",
    "            ).style(container=False)\n",
    "\n",
    "    txt.submit(add_text, [chatbot, txt], [chatbot, txt]).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5EZK-XPCwxt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
